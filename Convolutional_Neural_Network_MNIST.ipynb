{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils \n",
    "import numpy.random as npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyCNN:    \n",
    "    ### 1.___ Initialize the class :\n",
    "    \"\"\"\n",
    "    scope -> name of scope to use for the variables of the model\n",
    "    data  -> tf.data containing image and label\n",
    "    summmaries_dir -> directory to store sumaries in case we want to use tensorboard\n",
    "    \"\"\"\n",
    "    def __init__(self, scope='network_CNN', num_classes=10,\n",
    "                 summaries_dir=None, training = False, keep_prob=1):\n",
    "        # variables of the model :\n",
    "        self.scope = scope\n",
    "        self.keep_prob = keep_prob\n",
    "        self._training = training\n",
    "        self.num_classes = num_classes\n",
    "        # Write tensorboard summaries :\n",
    "        self.summary_writer = None\n",
    "        #build model within scope, update summaries if needed :\n",
    "        with tf.variable_scope(scope):\n",
    "            #get the graph :\n",
    "            self.__build_model()\n",
    "            if summaries_dir :\n",
    "                # Get the directory for the summaries :\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                # if directory doesnt exist -> create it :\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                # Write summaries in summary_writer :\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "    \n",
    "    ### 2.____ Build the graph for the model :  \n",
    "    def  __build_model(self):         \n",
    "        #####_____________1. Placeholders for data _____________________________________\n",
    "        self.img = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.float32, name=\"image\")\n",
    "        self.label = tf.placeholder(shape=[10], dtype=tf.float32, name=\"label\")\n",
    "        \n",
    "        #####_____________2. Design the graph __________________________________________\n",
    "        with tf.variable_scope('CONV1', reuse=tf.AUTO_REUSE) as scope:\n",
    "            conv1 = tf.layers.conv2d(self.img, filters = 24, kernel_size = [3, 3], \\\n",
    "                                padding='SAME', activation=tf.nn.relu,\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv1')\n",
    "        # Second Convolutional lyer :\n",
    "        with tf.variable_scope('CONV2', reuse=tf.AUTO_REUSE) as scope:\n",
    "            conv2 = tf.layers.conv2d(conv1, filters = 12, kernel_size = [3, 3], \\\n",
    "                                padding='SAME', activation=tf.nn.relu,\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer(), name='conv2')\n",
    "        # Third a Fully-connected layer :\n",
    "        with tf.variable_scope('FC1', reuse=tf.AUTO_REUSE) as scope:\n",
    "            flattened = tf.contrib.layers.flatten(conv2)\n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        # Dropout :\n",
    "        with tf.variable_scope('DROPOUT', reuse=tf.AUTO_REUSE) as scope:\n",
    "            fc1 = tf.layers.dropout(fc1, self.keep_prob, training=self._training, name='Dropout') \n",
    "            \n",
    "        with tf.variable_scope('FC2', reuse=tf.AUTO_REUSE) as scope:\n",
    "            fc2 = tf.contrib.layers.fully_connected(fc1, 10)\n",
    "            self.logits_ = fc2\n",
    "        \n",
    "        with tf.variable_scope('probabilities', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.probabilities = tf.nn.softmax(self.logits_)\n",
    "        \n",
    "        #####_____________3. Get the loss and define the optimization operation : ______\n",
    "        # define the loss :\n",
    "        with tf.variable_scope('loss', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( \\\n",
    "                                    logits=self.logits_, labels=self.label))\n",
    "        \n",
    "        with tf.variable_scope('OPT', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n",
    "            self.train = self.optimizer.minimize(self.loss)\n",
    "                \n",
    "        # _____________4. Summaries for Tensorboard : __________________________________\n",
    "        self.summaries = tf.summary.merge([tf.summary.scalar(\"loss\", self.loss)])\n",
    "        \n",
    "        \n",
    "    ### 3.____ Prediction function :\n",
    "    \"\"\"\n",
    "    sess -> tf.Session() to run the graph\n",
    "    images -> set of images that we want to classify\n",
    "    \"\"\"\n",
    "    def predict(self, images):\n",
    "        self.keep_prob = 1\n",
    "        self._training = False \n",
    "        with tf.Session() as sess :\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            prediction = sess.run(tf.argmax(self.probabilities, axis=1), {self.img: images})\n",
    "            return prediction    \n",
    "    \n",
    "    \"\"\"\n",
    "    __train_step : one training step fct to use for fitting the data\n",
    "    inputs :\n",
    "        images, labels = data to train with\n",
    "    \"\"\"\n",
    "    def __train_step(self, sess, images, labels):\n",
    "        \n",
    "        self._training = True\n",
    "        feed_dict = {self.img: images, self.label: labels}\n",
    "        summaries, _, loss = sess.run([self.summaries, self.train, self.loss], feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries)                      \n",
    "        return loss            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "dataset = [npr.randint(0, 200, [100, 84, 84, 4]), npr.randint(0, 10, [100])]\n",
    "print(len(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_next_batch(data, batch_size):\n",
    "    \n",
    "    len_data = len(dataset[0])\n",
    "    perm = npr.permutation(range(len_data))\n",
    "    indices = perm[:batch_size]\n",
    "    images = [ dataset[0][i] for i in indices ]\n",
    "    labels = [ dataset[1][i] for i in indices ]\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(dataset, n_epochs=10, n_batches=100, batch_size=24, \n",
    "        save_=True, save_epoch=10, save_directory='checkpoint_directory/MyCNN'):\n",
    "        \n",
    "        if save_:\n",
    "            # create a saver object\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        with tf.Session() as sess :            \n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for i in range(n_epochs):\n",
    "                \n",
    "                batch=0 #initiaize batch to zero\n",
    "                while batch<n_batches :                    \n",
    "                    images, labels = get_next_batch(dataset, batch_size=batch_size)\n",
    "                    loss_ = self.__train_step(sess, images, labels)\n",
    "                    total_loss += loss_\n",
    "                    batch += 1\n",
    "                # print total loss for the select epoch :    \n",
    "                print('Average loss epoch {0}: {1}'.format(i, total_loss/batche))\n",
    "                \n",
    "                # save if (epoch+1)%10 = 0\n",
    "                \n",
    "                if save_ and (i+1)%save_epoch == 0:\n",
    "                    saver.save(sess, save_directory, global_step=i)                    \n",
    "                \n",
    "            # print the running time :    \n",
    "            print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
